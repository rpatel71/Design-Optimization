{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXRMKyVDqIEjq9xmoSfv41",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpatel71/Design-Optimization/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Yjec49taIQr"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch as t\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd.functional import jacobian"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fun =lambda x: x[0] ** 2 + x[1] ** 2 + x[2] ** 2  # Objective function\n",
        "const1 = lambda x: ((x[0] ** 2) / 4) + ((x[1] ** 2) / 5) + ((x[2] ** 2) / 25) - 1 # Constraint 1 from the given constraints\n",
        "const2 = lambda x: x[0] + x[1] - x[2] # Constraint 2 from the given constaints\n",
        "\n",
        "x = Variable(t.tensor([1., 1., 1.]), requires_grad=True)\n",
        "E = 1e-03"
      ],
      "metadata": {
        "id": "H7IZhbCzaNP9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def red_grad(fun, const1, const2, x): # This function calculates the Reduced Gradient\n",
        "  # compute jacobian for the function and constraints\n",
        "  jac = t.zeros((3, 3))\n",
        "  jac[0] = jacobian(fun, (x))\n",
        "  jac[1] = jacobian(const1, (x))\n",
        "  jac[2] = jacobian(const2, (x))\n",
        "\n",
        "  # Variables we need, to calculate reduced function\n",
        "  dfdd = jac[0,0]\n",
        "  dfds = jac[0,1:]\n",
        "  dhds = jac[1:,1:]\n",
        "  dhdd = jac[1:,0]\n",
        "\n",
        "  # Finding the reduced gradient\n",
        "  inv_dhds = t.pinverse(dhds)\n",
        "  dfds_inv_dhds = t.matmul(dfds, inv_dhds)\n",
        "  red_g = dfdd - t.matmul(dfds_inv_dhds, dhdd)\n",
        "\n",
        "  return red_g, dfdd, dfds, dhds, dhdd"
      ],
      "metadata": {
        "id": "rdTRswtHaQFD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function of Levenberg -Marquardt Algorithm\n",
        "def Lev_Marq(x):\n",
        "  lam = 1.\n",
        "  mod = t.norm(t.tensor([const1(x),const2(x)]))\n",
        "  while mod > 1e-06:\n",
        "    red_g, dfdd, dfds, dhds, dhdd = red_grad(fun, const1, const2, x)\n",
        "    with t.no_grad():\n",
        "      inv = t.pinverse(t.matmul(dhds.T, dhds) + lam * t.eye(2))\n",
        "      calc = t.matmul(t.matmul(inv, dhds.T), t.tensor([const1(x),const2(x)]))\n",
        "      x[1:] = x[1:] - calc\n",
        "    mod = t.norm(t.tensor([const1(x),const2(x)]))\n",
        "  return x\n",
        "\n",
        "\n",
        "# Defining a function which improves x for line search\n",
        "def imp_x(x, a):\n",
        "    x_imp = t.zeros(3)\n",
        "    red_g, dfdd, dfds, dhds, dhdd= red_grad(fun, const1, const2, x)\n",
        "    x_imp[0] = x[0] - a * red_g\n",
        "    x_imp[1:] = x[1:] + (a * (t.matmul(t.pinverse(dhds),dhdd))*red_g)\n",
        "    return x_imp\n",
        "\n",
        "\n",
        "# Define Line search algorithm\n",
        "def line_search(x, max_iter=25):\n",
        "    red_g, dfdd, dfds, dhds, dhdd= red_grad(fun, const1, const2, x)\n",
        "    a = 1 # Alpha\n",
        "    iter = 0  # iterations\n",
        "    tr=0.5\n",
        "    f = fun(imp_x(x, a))\n",
        "    phi = fun(x) - (tr * a * (red_g ** 2))\n",
        "    while f > phi and iter < max_iter:\n",
        "        a = 0.5 * a\n",
        "        f = fun(imp_x(x, a))\n",
        "        phi = fun(x)- (tr * a * (red_g ** 2))\n",
        "        iter += 1\n",
        "    return a"
      ],
      "metadata": {
        "id": "Ug8t2etqaSTl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Generalized Reduced Gradient function\n",
        "def gen_red_grad(x, E = 1e-03):\n",
        "  x_num = x.detach().numpy()\n",
        "  print(f'Here, the initial value of x is: {x_num} at iteration 0.')\n",
        "  x = Lev_Marq(x)\n",
        "  print(f'\\n the feasible value of x is: {x} at iteration 0')\n",
        "  x_num = np.vstack((x_num, x.detach().numpy()))\n",
        "  obj_fun = [fun(x).item()]\n",
        "  a_val = [1]\n",
        "  red_g, dfdd, dfds, dhds, dhdd = red_grad(fun, const1, const2, x)\n",
        "  err = t.norm(red_g)\n",
        "  error = [err]\n",
        "  iter = 0\n",
        "  while err > E:\n",
        "    a = line_search(x, 25)\n",
        "    red_g, dfdd, dfds, dhds, dhdd = red_grad(fun, const1, const2, x)\n",
        "    with t.no_grad():\n",
        "      x[0] = x[0] - a * red_g\n",
        "      inv_dhds = t.pinverse(dhds)\n",
        "      x[1:] = x[1:] + (a * np.matmul(inv_dhds, dhdd) * red_g)\n",
        "      x = Lev_Marq(x)\n",
        "      err = t.norm(red_g)\n",
        "      x_num = np.vstack((x_num, x.detach().numpy()))\n",
        "      obj_fun.append(fun(x).item())\n",
        "      a_val.append(a)\n",
        "      error.append(err)\n",
        "      iter += 1\n",
        "  return x_num, obj_fun, a_val, error, iter"
      ],
      "metadata": {
        "id": "D-1ShdfBaUpq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_num, obj_fun, a_val, error, iter = gen_red_grad(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8HFC8RgaXcD",
        "outputId": "333251c1-f3c1-40b6-b4f7-3914a9ba57a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here, the initial value of x is: [1. 1. 1.] at iteration 0.\n",
            "\n",
            " the feasible value of x is: tensor([1.0000, 1.5614, 2.5614], requires_grad=True) at iteration 0\n"
          ]
        }
      ]
    }
  ]
}